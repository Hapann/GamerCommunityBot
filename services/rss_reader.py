# bot/services/rss_reader.py
import aiohttp
import feedparser
import asyncio
from datetime import datetime
from logger.logger import logger

RSS_FEEDS = [
    "https://www.goha.ru/rss/news",
    "https://www.playground.ru/rss/news.xml",
    "https://dtf.ru/rss/all"
]
#https://www.gamedeveloper.com/rss.xml - eng
#https://www.uploadvr.com/feed/ - eng
#https://www.roadtovr.com/feed/ - eng
#https://www.polygon.com/feed/ - eng
#https://www.pcgamer.com/rss/ - eng
#https://www.gamespot.com/feeds/news/ - eng
#https://kotaku.com/feed - eng

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

async def fetch_feed_simple(session, url):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ RSS-—Ñ–∏–¥–∞."""
    logger.info(f"üîÑ –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url}")
    try:
        async with session.get(url, headers=HEADERS, timeout=10) as response:
            if response.status != 200:
                logger.warning(f"‚ùå {url} - —Å—Ç–∞—Ç—É—Å {response.status}")
                return []

            content = await response.text()
            logger.info(f"‚úÖ {url} - –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ü–∞—Ä—Å–∏–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            parsed = feedparser.parse(content)
            logger.info(f"üìä {url} - —Ä–∞—Å–ø–∞—Ä—à–µ–Ω–æ {len(parsed.entries)} –∑–∞–ø–∏—Å–µ–π")

            items = []
            for entry in parsed.entries[:5]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5
                if hasattr(entry, 'title') and hasattr(entry, 'link'):
                    items.append({
                        "title": entry.title,
                        "link": entry.link,
                        "source": url
                    })
                    logger.info(f"üì∞ –î–æ–±–∞–≤–ª–µ–Ω–∞: {entry.title[:30]}...")

            return items

    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞ —Å {url}: {e}")
        return []

async def get_all_rss_news():
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π."""
    logger.info("üéØ –ó–ê–ü–£–°–ö get_all_rss_news()")

    try:
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_feed_simple(session, url) for url in RSS_FEEDS]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        all_items = []
        for result in results:
            if isinstance(result, list):
                all_items.extend(result)

        logger.info(f"üéâ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢: {len(all_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        if all_items:
            for i, item in enumerate(all_items[:3]):
                logger.info(f"üèÜ –ü—Ä–∏–º–µ—Ä {i+1}: {item['title'][:50]}...")

        return all_items

    except Exception as e:
        logger.exception(f"üí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        return []
